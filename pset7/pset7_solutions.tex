\documentclass[11pt,oneside]{amsart}
\usepackage{geometry}
\usepackage{amssymb,parskip,mathtools,microtype}
\usepackage[shortlabels]{enumitem}
\usepackage[most]{tcolorbox}

\definecolor{sol}{rgb}{0.1, 0.3, 0.6}

\newtcolorbox{solution}{enhanced, breakable, colframe=sol, title=Solution}

\theoremstyle{definition}
\newtheorem{problem}{Problem}

\newcommand{\bC}{\mathbb{C}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\eps}{\varepsilon}
\newcommand*\colvec[1]{\begin{psmallmatrix}#1\end{psmallmatrix}}
\newcommand*\dcolvec[1]{\begin{pmatrix}#1\end{pmatrix}}

\DeclareMathOperator{\Span}{span}
\let\Re\relax
\DeclareMathOperator{\Re}{Re}
\let\Im\relax
\DeclareMathOperator{\Im}{Im}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\rank}{rank}

\title{MATH2211 Spring 2022\\
Problem Set 7 Solutions}
\author{Due Wednesday, March 30, 2022 at 11:59 pm}

\begin{document}
    \maketitle
    
    \begin{problem}
        Suppose $A\in M_{m\times n}(F)$. Prove or find counterexamples:
        \begin{enumerate}[(a)]
            \item For any $B\in M_{n\times p}(F)$, $\rank(AB)\leq\rank(A)$.
            \item For any $B\in M_{p\times m}(F)$, $\rank(BA)\leq\rank(A)$.
        \end{enumerate}
        \begin{solution}
            Here is an incredibly short and concise reason why both statements are true. The image of a vector space under a matrix always has dimension less than or equal to the vector space you started with, no matter what the matrix is. Moreover, the rank of a matrix is at most the dimension of the source space.

            Some notation: For any linear transformation $T\colon V\to W$ and for any subspace $U\subseteq V$, we write $TU$ to mean the space $\{Tv:v\in U\}$. This is also the image of $T|_U$, the restriction of $T$ to $U$.

            Let $V=F^p$ in part (a). Then $BV=\im B\subseteq F^n$, thus $ABV\subseteq AF^n=\im A$. Therefore, $\rank(AB)=\dim(ABV)\leq\dim(AF^n)=\rank A$.

            Let $V=F^m$ in part (b). Then $AV$ has dimension $(\rank A)$, and so $\dim(BAV)=\im(B|_{AV})\leq \dim(AV)=\rank A$.
        \end{solution}
    \end{problem}

    \begin{problem}
        \leavevmode\begin{enumerate}[(a)]
            \item Prove, using the exterior algebra definition of the determinant, that the determinant of an upper-triangular square matrix is the product of the diagonal entries.
            \begin{solution}
                Let $(a_{ij})_{1\leq i,j\leq n}$ be an $n\times n$ matrix.  The wedge product of the columns of this matrix is
                \[(a_{11}e_1)\wedge(a_{12}e_1+a_{22}e_2)\wedge\cdots\wedge(a_{1n}e_1+\cdots+a_{nn}e_n).\]
                Because the first term is a scalar multiple of $e_1$, the $e_1$ coefficients in the rest of the terms do not contribute to the final expanded wedge product. Therefore, the above $n$-vector is equal to
                \[(a_{11}e_1)\wedge(a_{22}e_2)\wedge(a_{23}e_2+a_{33}e_3)\wedge\dots\wedge (a_{2n}e_2+\cdots+a_{nn}e_n).\]
                We can continue this argument until the $n$-vector simplifies to
                \[a_{11}a_{22}\cdots a_{nn}\cdot e_1\wedge\cdots\wedge e_n.\]
                Thus the determinant of $(a_{ij})_{1\leq i,j\leq n}$ is equal to $a_{11}a_{22}\cdots a_{nn}$.

                This is sort of an ``unrigorous'' way to do induction. But I think this is acceptable as long as it's easy to check that it can be turned into a rigorous proof by induction.
            \end{solution}
            \item You may assume that a similar proof as in part (a) shows that the same result holds for lower triangular square matrices as well.
            
            Prove that that the elementary matrix $E$ representing the row operation ``add a multiple of row $j$ to row $i$'' ($i\neq j$) has determinant 1.
            \begin{solution}
                The given elementary matrix $E$ has 1 along the diagonal and 0 everywhere else except for a single nonzero entry somewhere above or below the diagonal. This makes $E$ upper or lower triangular, respectively, so part (a) shows that the determinant of $E$ is 1.
            \end{solution}
        \end{enumerate}
    \end{problem}

    \begin{problem}
        \leavevmode\begin{enumerate}[(a)]
            \item Prove that if $T\colon V\to W$ is surjective, then $^tT\colon W^*\to V^*$ is injective.
            \begin{solution}
                Suppose that $^tT\mu=0$ for some $\mu\in W^*$. Thus $\mu T=0$. Saying that $\mu T$ is the zero linar functional on $V$ is the same thing as saying that $\mu Tv=0$ for all $v\in V$. Since $T$ is surjective, we have $\{Tv:v\in V\}=W$. Thus $\mu w=0$ for all $w\in W$. Thus $\mu$ is the zero linear functional. Therefore, $^tT$ is injective.
            \end{solution}
            \item Prove that if $T\colon V\to W$ is injective, then $^tT\colon W^*\to V^*$ is surjective.
            \begin{solution}
                Here's a somewhat clean proof avoiding bases. The injectivity of $T$ implies that $T$ is an isomorphism of $V$ onto its image, which we call $TV$.

                To show that $^tT$ is surjective, let's take an arbitrary $\lambda\in V^*$ and try to show that there exists $\mu\in W^*$ such that $\mu T=\lambda$. Let us split $W$ into a direct sum $W=TV\oplus Z$, hence every vector $w\in W$ is uniquely a sum $Tv+z$ for some $v\in V$ and $z\in Z$. This mapping is well defined because $T$ is an isomorphism. Let us define $\mu\colon W\to\bR$ by $\mu(Tv+z)=\lambda(v)$ for all $v\in V$, $z\in Z$. One easily checks that $\mu$ is a linear functional and that $\mu Tv=\lambda v$ for all $v\in V$, showing that $\mu T=\lambda$.

                Note: this isn't the only $\mu$ that can be chosen. We could have chosen $\mu(Tv+z)=\lambda(v)+\eta(z)$ for any linear functional $\eta$ on $Z$. This corresponds to the fact that $^tT$ is not necessarily injective.
            \end{solution}
            \item Prove that every linear transformation $T\colon V\to W$ can be factored as a composition
            \[V\overset{T'}{\twoheadrightarrow} U\overset{i}{\hookrightarrow}W,\]
            where $T'$ is surjective and $i$ is injective. (Hint: Set $U=\im T$.)
            \begin{solution}
                The hint basically solves everything. Set $U=\im T$, set $T'$ to be the codomain restriction of $T$ to $\im T$, and let $i$ be the inclusion $\im T\hookrightarrow W$. By definition of $\im T$, every vector in $\im T$ is equal to $Tv$ (which is also $T'v$) for some $v\in V$, so $T'$ is surjective. Moreover, $i$ is clearly injective because it is an inclusion map.
            \end{solution}
            \item Recall that the rank of a linear transformation $T\colon V\to W$ is defined to be $\dim\im T$. Use parts (a), (b), and (c) to prove that $\rank(T)=\rank(^tT)$.
            \begin{solution}
                The only ingredient needed to finish the proof is that the dimension of the middle term in any surjective-injective factorization (i.e.\ some other factorization besides the one with $U=\im T$ as in part (c)) depends only on $T$ and not on the factorization. This is because $\dim U=\dim(iU)=\dim(\im T)$ by the fact that $T=iT'$.

                Now let us prove that $\rank(T)=\rank(^tT)$. We can observe two factorizations of $^tT$ into surjective and injective maps. The first one is
                \[W^*\twoheadrightarrow \im(^tT)\hookrightarrow V^*\]
                obtained by applying part (c) to $^tT$. The second one is
                \[W^*\overset{^ti}\twoheadrightarrow (\im T)^*\overset{^tT'}\hookrightarrow V^*\]
                obtained by dualizing
                \[V\overset{T'}{\twoheadrightarrow} \im T\overset{i}{\hookrightarrow}W\]
                and using parts (a) and (b) to see that $^ti$ is surjective and $^tT'$ is injective. Therefore, using the ingredient in the first paragraph, we find that $\dim ((\im T)^*)=\dim \im(^tT)$. But we already know that $\dim((\im T)^*)=\dim(\im T)$, so therefore have proved that $\rank(T)=\rank(^tT)$.
            \end{solution}
        \end{enumerate}
    \end{problem}

    \begin{problem}
        Let $V$ be a vector space. Given a nonzero element $v\in V$ and a nonzero linear functional $\lambda\colon V\to \bR$, we can make a linear transformation $T_{v,\lambda}\colon V\to V$ by sending $x\in V$ to $\lambda(x)\cdot v$. Prove that $T_{v,\lambda}$ has rank 1, and prove that every rank 1 linear transformation from $V$ to $V$ is equal to $T_{v,\lambda}$ for some $0\neq v\in V$ and $0\neq \lambda\colon V\to \bR$.

        Hint for the second part: Problem 3(c) is very helpful.
    \end{problem}
    \begin{solution}
        The linear transformation $T_{v,\lambda}$ has rank 1 because the image of $T_{v,\lambda}$ consists of scalar multiples of $v$ (and these scalar multiples are nonzero because $\lambda$ is not the zero linear functional.)

        Let $T$ now be a rank 1 matrix. Therefore, the image of $T$ is a 1-dimensional subspace of $V$. Let $\{v\}$ be any basis of this subspace. For each $w\in V$, we know that $Tw$ is some multiple of $v$. Let us define a function $\lambda(w)\in F$ defined by $Tw=\lambda(w)v$ for all $w\in V$. Now one easily checks that in fact $\lambda$ depends linearly on $w$, i.e.\ is a linear transformation, so $\lambda$ is a linear functional. Therefore, $T=T_{v,\lambda}$ for this $v$ and this $\lambda$.
    \end{solution}

    \begin{problem}
        Let $\frac d{dx}\colon P_2(\bR)\to P_2(\bR)$ be the derivative operator on the space of polynomials of degree at most 2 over $\bR$.
        \begin{enumerate}[(a)]
            % \item Let $(1,x,x^2)$ be an ordered basis of $P_2(\bR)$. Let $(\eps_0,\eps_1,\eps_2)$ be the corresponding ordered dual basis of $P_2(\bR)^*$. Prove that
            % \[\eps_i=(P_2(\bR)\ni p\mapsto \text{the coefficient of $x^i$ in }p)\in\bR.\]
            \item Prove that $^t(\frac d{dx})\colon P_2(\bR)^*\to P_2(\bR)^*$ has a 1-dimensional kernel.
            \begin{solution}
                Using Problem 3 we find that $2=\rank \frac d{dx}=\rank(^t(\frac d{dx}))$. Thus the nullity of $^t(\frac d{dx})$ is $3-2=1$.
            \end{solution}
            \item Prove that $\ker(^t(\frac d{dx}))$ is spanned by the functional taking a polynomial to its $x^2$ coefficient. Can you provide a plain English interpretation of what this is saying?
            \begin{solution}
                This can be done with matrices, but here is a neater proof. First, write $[x^2]$ as the linear functional $P_2(\bR)\to F; p\mapsto \text{the coefficient of }x^2\text{ in }p$. This just lets us have a convenient name to call this functional.
                
                For any polynomial $p\in P_2(\bR)$, we have $(^t(\frac d{dx})[x^2])p=[x^2]\frac{dp}{dx}$. Now, the derivative of a polynomial of degree 2 has degree 1, so $[x^2]\frac{dp}{dx}=0$. Hence $[x^2]\in\ker (^t(\frac d{dx}))$. By part (a), the kernel is one-dimensional, so we are done.

                The English interpretation of the fact that $[x^2]$ spans the kernel of the transpose of the derivative operator is that the derivative of any polynomial in $P_2(\bR)$ always has $x^2$ coefficient equal to 0. This is basically what we used in the proof above, but is a new interpretation if you solved this problem with matrices.

                It's quite interesting to see that analyzing $\ker\frac d{dt}$ and $\ker(^t(\frac d{dt}))$ show two very different facts about the derivative!
            \end{solution}
        \end{enumerate}
    \end{problem}
\end{document}
